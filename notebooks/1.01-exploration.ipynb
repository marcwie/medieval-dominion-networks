{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320d2912-e065-45ae-bf1f-42bf2524234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import hydra\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize hydra and move to the root of the repository\n",
    "try:\n",
    "    hydra.initialize(version_base=None, config_path=\"../config/\")\n",
    "    CONFIG = hydra.compose(config_name=\"main.yaml\")\n",
    "    print('Initializing hydra')\n",
    "except:\n",
    "    print('Hydra already initalized!')\n",
    "else:\n",
    "    # Create an output folder in the root of the repository\n",
    "    os.chdir('..')\n",
    "    OUTPUT_FOLDER = Path('output/{0}'.format(datetime.datetime.now()))\n",
    "    Path(OUTPUT_FOLDER).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f8911e-a530-4815-bbca-1a0b7f04f853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from src.utils.styling import hide_and_move_axis\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e2a9fd-95fd-41b8-b67c-12a7f17c4506",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c2bf38-b9ce-42ea-822f-468479186243",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.read_csv('data/01_raw/city_nodes_annual_all.csv')\n",
    "edges = pd.read_csv('data/01_raw/city_edges_annual_all.csv')\n",
    "\n",
    "pos = nodes[['PlaceID', 'XCOORD', 'YCOORD']].drop_duplicates().set_index('PlaceID').to_dict('index')\n",
    "pos = {key: (value['XCOORD'], value['YCOORD']) for key, value in pos.items()}\n",
    "\n",
    "years = np.sort(edges.Year.unique())\n",
    "places = nodes.PlaceID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6f1e73-86eb-4b70-870e-c27420cb6711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_network():\n",
    "    edge_weights = edges.groupby(['from', 'to']).Year.count().reset_index().rename(columns={'Year': 'weight'})\n",
    "    edge_weights.weight /= edge_weights.weight.max()\n",
    "    edge_weights = edge_weights.values.tolist()\n",
    "    edge_weights = [[int(e[0]), int(e[1]), e[2]] for e in edge_weights]\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    G.add_weighted_edges_from(edge_weights)\n",
    "\n",
    "    return G\n",
    "\n",
    "def singular_network(year, edges):\n",
    "    edges = edges[edges.Year == year]\n",
    "    edges = edges[['from', 'to']].values.tolist()\n",
    "    return edges\n",
    "\n",
    "def compute_coincidences():\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for i, j in combinations(places, 2):\n",
    "        scalar1 = nodes[nodes.PlaceID == i][['Year', 'Juden']]\n",
    "        scalar2 = nodes[nodes.PlaceID == j][['Year', 'Juden']]\n",
    "        \n",
    "        scalar1 = scalar1.sort_values(by='Year')\n",
    "        scalar1[['dYear', 'dJews']] = scalar1.diff()\n",
    "        \n",
    "        scalar2 = scalar2.sort_values(by='Year')\n",
    "        scalar2[['dYear', 'dJews']] = scalar2.diff()\n",
    "        \n",
    "        scalar1 = scalar1[scalar1.dYear == 1]\n",
    "        scalar2 = scalar2[scalar2.dYear == 1]\n",
    "        \n",
    "        scalar = pd.merge(scalar1, scalar2, on='Year')\n",
    "        scalar.dJews_x = scalar.dJews_x == 1\n",
    "        scalar.dJews_y = scalar.dJews_y == 1\n",
    "        \n",
    "        norm = np.max([scalar.dJews_x.sum(), scalar.dJews_y.sum()])\n",
    "        coicidence = (scalar.dJews_x * scalar.dJews_y)\n",
    "        count = coicidence.sum()\n",
    "    \n",
    "        if count != 0:\n",
    "            print(i, j, count, norm, scalar[coicidence].Year.values, end='\\r')\n",
    "            rate = count / norm\n",
    "        else:\n",
    "            rate = 0\n",
    "        results.append([i, j, count, norm, rate, scalar[coicidence].Year.values])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def compute_similarity_absence_presence():\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for i, j in combinations(places, 2):\n",
    "\n",
    "        print(i, places.max(), end='\\r')\n",
    "        \n",
    "        scalar1 = nodes[nodes.PlaceID == i][['Year', 'Juden']]\n",
    "        scalar2 = nodes[nodes.PlaceID == j][['Year', 'Juden']]\n",
    "        scalar = pd.merge(scalar1, scalar2, on='Year')\n",
    "        scalar = scalar.Juden_x == scalar.Juden_y\n",
    "        scalar = scalar.sum() / len(scalar)\n",
    "        results.append([i, j, scalar])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def compute_similarity_presence():\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for i, j in combinations(places, 2):\n",
    "\n",
    "        print(i, places.max(), end='\\r')\n",
    "        \n",
    "        scalar1 = nodes[nodes.PlaceID == i][['Year', 'Juden']]\n",
    "        scalar2 = nodes[nodes.PlaceID == j][['Year', 'Juden']]\n",
    "        scalar = pd.merge(scalar1, scalar2, on='Year')\n",
    "        scalar = scalar.Juden_x * scalar.Juden_y\n",
    "        scalar = scalar.sum() / len(scalar)\n",
    "        results.append([i, j, scalar])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28abfc2b-e048-445e-b995-dcbeac64252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_edges = []\n",
    "for year in years:\n",
    "    e = singular_network(year, edges)\n",
    "    n_edges.append(len(e))\n",
    "    \n",
    "f, ax0 = plt.subplots()\n",
    "nodes.groupby('Year').Juden.sum().plot(ax=ax0)\n",
    "ax1 = ax0.twinx()\n",
    "ax1.plot(years, n_edges, c='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56173ae2-dc78-43ba-8701-5902927c6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = weighted_network()\n",
    "comms = nx.community.louvain_communities(G)\n",
    "T = np.sort([len(c) for c in comms])[-10]\n",
    "comms = [c for c in comms if len(c) >= T]\n",
    "assert len(comms) <= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbb65b6-e02b-481f-917a-481a981b6361",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [G.get_edge_data(u, v)['weight'] for u, v in G.edges()]\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.hist(weights, bins=np.arange(0, 1, 0.05), width=0.04)\n",
    "\n",
    "ax.set_xlabel('Edge weight')\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "hide_and_move_axis(ax)\n",
    "plt.savefig(OUTPUT_FOLDER / 'edge_weight_histrogram.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec843333-ea97-4bc4-9b83-d2ba8cf938a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_networkx_nodes(G, pos, node_size=12, node_color='k')\n",
    "\n",
    "for i, comm in enumerate(comms):\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=comm, node_size=15, node_color=colors[i])\n",
    "\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.025)\n",
    "plt.savefig(OUTPUT_FOLDER / 'communities.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc344a9-b561-4a96-a5d1-55dcc7c3456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    coincidences_df = pd.read_feather('computations/coincidences.feather')\n",
    "except:\n",
    "    coincidences = compute_coincidences()\n",
    "    coincidences_df = pd.DataFrame(coincidences, columns=['id1', 'id2', 'count', 'norm', 'rate', 'years'])\n",
    "    coincidences_df.set_index(['id1', 'id2'], inplace=True)\n",
    "    coincidences_df.to_feather('computations/coincidences.feather')\n",
    "else:\n",
    "    print('Read data from disk!')\n",
    "    \n",
    "try:\n",
    "    similarity_absence_presence_df = pd.read_feather('computations/similarity_absence_presence.feather')\n",
    "except:\n",
    "    similarity_absence_presence = compute_similarity_absence_presence()\n",
    "    similarity_absence_presence_df = pd.DataFrame(similarity_absence_presence, columns=['id1', 'id2', 'rate'])\n",
    "    similarity_absence_presence_df.set_index(['id1', 'id2'], inplace=True)\n",
    "    similarity_absence_presence_df.to_feather('computations/similarity_absence_presence.feather')\n",
    "else:\n",
    "    print('Read data from disk!')\n",
    "    \n",
    "try:\n",
    "    similarity_presence_df = pd.read_feather('computations/similarity_presence.feather')\n",
    "except:\n",
    "    similarity_presence = compute_similarity_presence()\n",
    "    similarity_presence_df = pd.DataFrame(similarity_presence, columns=['id1', 'id2', 'rate'])\n",
    "    similarity_presence_df.set_index(['id1', 'id2'], inplace=True)\n",
    "    similarity_presence_df.to_feather('computations/similarity_presence.feather')\n",
    "else:\n",
    "    print('Read data from disk!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59337da6-ddbe-4ba1-954d-b2ec7c76182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefficients(nodes, similarity, key='count'):\n",
    "\n",
    "    nodes = np.sort(list(nodes))\n",
    "    indices = list(combinations(nodes, 2))\n",
    "    return similarity[coincidences_df.index.isin(indices)][key]\n",
    "\n",
    "def cumulative(results):\n",
    "\n",
    "    x = np.sort(results)\n",
    "    N = len(results)\n",
    "    y = np.arange(N) / float(N) \n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1313c0dc-b5db-4c07-bf2f-91c68a6f5ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(2, 5, figsize=(10, 5), sharex=True, sharey=True)\n",
    "flatax = axarr.flatten()\n",
    "\n",
    "which = 'presence'\n",
    "\n",
    "if which == 'absencepresence':\n",
    "    SIMILARITY = similarity_absence_presence_df\n",
    "    KEY = 'rate'\n",
    "    axarr[1, 2].set_xlabel('Share of shared years with Jews present OR absent')\n",
    "elif which == 'presence':\n",
    "    SIMILARITY = similarity_presence_df\n",
    "    KEY = 'rate'\n",
    "    axarr[1, 2].set_xlabel('Share of shared years with Jews present')\n",
    "else:\n",
    "    assert False\n",
    "    \n",
    "for i in range(10):\n",
    "    print(i, end='\\r')\n",
    "    community = comms[i]\n",
    "    \n",
    "    for _ in range(200):\n",
    "        results = get_coefficients(np.random.choice(places, len(community), replace=False), SIMILARITY, KEY)\n",
    "        x, y = cumulative(results)\n",
    "        flatax[i].plot(x, y, alpha=0.1, c='k') \n",
    "    \n",
    "    results = get_coefficients(community, SIMILARITY, KEY)\n",
    "    x, y = cumulative(results)\n",
    "    \n",
    "    flatax[i].plot(x, y, lw=3, color=colors[i]) \n",
    "\n",
    "for ax in axarr[:, 0]:\n",
    "    ax.set_ylabel('Cumulative distribution function')\n",
    "\n",
    "#for ax in axarr[1]:\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_FOLDER / f'cumulative_distributions_{which}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b9fb5c-17e1-48a7-8766-9fe2d3bc0031",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMILARITY = coincidences_df\n",
    "KEY = 'count'\n",
    "\n",
    "f, axarr = plt.subplots(2, 5, figsize=(8, 4), sharex=True, sharey=True)\n",
    "flatax = axarr.flatten()\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    community = comms[i]\n",
    "    frequencies = []\n",
    "    \n",
    "    for _ in range(500):\n",
    "        results = get_coefficients(np.random.choice(places, len(community), replace=False), SIMILARITY, KEY)\n",
    "        frequencies.append((results > 0).mean())\n",
    "    flatax[i].hist(frequencies, color='k', bins=np.arange(0, 0.15, 0.005), alpha=0.25)\n",
    "    \n",
    "    results = get_coefficients(community, SIMILARITY, KEY)\n",
    "    results = (results > 0).mean()\n",
    "    flatax[i].axvline(results, color=colors[i], lw=3)\n",
    "    \n",
    "    frequencies = np.array(frequencies)\n",
    "    print(i, (frequencies > results).mean())\n",
    "\n",
    "    if (frequencies > results).mean() < 0.05:\n",
    "        flatax[i].text(0.13, 2, \"*\", size=20)\n",
    "\n",
    "for ax in axarr[:, 0]:\n",
    "    ax.set_ylabel('Count')\n",
    "\n",
    "for ax in axarr[1]:\n",
    "    ax.set_xlabel('Share of\\nco-explusion events')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_FOLDER / 'coexpulsion_frequencies.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
