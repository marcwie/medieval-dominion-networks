{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320d2912-e065-45ae-bf1f-42bf2524234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import hydra\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize hydra and move to the root of the repository\n",
    "try:\n",
    "    hydra.initialize(version_base=None, config_path=\"../config/\")\n",
    "    CONFIG = hydra.compose(config_name=\"main.yaml\")\n",
    "    print('Initializing hydra')\n",
    "except:\n",
    "    print('Hydra already initalized!')\n",
    "else:\n",
    "    # Create an output folder in the root of the repository\n",
    "    os.chdir('..')\n",
    "    OUTPUT_FOLDER = Path('output/{0}'.format(datetime.datetime.now()))\n",
    "    Path(OUTPUT_FOLDER).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f8911e-a530-4815-bbca-1a0b7f04f853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from src.utils.styling import hide_and_move_axis\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e2a9fd-95fd-41b8-b67c-12a7f17c4506",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c2bf38-b9ce-42ea-822f-468479186243",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.read_csv('data/01_raw/city_nodes_annual_all.csv')\n",
    "edges = pd.read_csv('data/01_raw/city_edges_annual_all.csv')\n",
    "\n",
    "pos = nodes[['PlaceID', 'XCOORD', 'YCOORD']].drop_duplicates().set_index('PlaceID').to_dict('index')\n",
    "pos = {key: (value['XCOORD'], value['YCOORD']) for key, value in pos.items()}\n",
    "\n",
    "years = np.sort(edges.Year.unique())\n",
    "places = nodes.PlaceID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6f1e73-86eb-4b70-870e-c27420cb6711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_network():\n",
    "    edge_weights = edges.groupby(['from', 'to']).Year.count().reset_index().rename(columns={'Year': 'weight'})\n",
    "    edge_weights.weight /= edge_weights.weight.max()\n",
    "    edge_weights = edge_weights.values.tolist()\n",
    "    edge_weights = [[int(e[0]), int(e[1]), e[2]] for e in edge_weights]\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    G.add_weighted_edges_from(edge_weights)\n",
    "\n",
    "    return G\n",
    "\n",
    "def singular_network(year, edges):\n",
    "    edges = edges[edges.Year == year]\n",
    "    edges = edges[['from', 'to']].values.tolist()\n",
    "    return edges\n",
    "\n",
    "def compute_coincidences():\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for i, j in combinations(places, 2):\n",
    "        scalar1 = nodes[nodes.PlaceID == i][['Year', 'Juden']]\n",
    "        scalar2 = nodes[nodes.PlaceID == j][['Year', 'Juden']]\n",
    "        \n",
    "        scalar1 = scalar1.sort_values(by='Year')\n",
    "        scalar1[['dYear', 'dJews']] = scalar1.diff()\n",
    "        \n",
    "        scalar2 = scalar2.sort_values(by='Year')\n",
    "        scalar2[['dYear', 'dJews']] = scalar2.diff()\n",
    "        \n",
    "        scalar1 = scalar1[scalar1.dYear == 1]\n",
    "        scalar2 = scalar2[scalar2.dYear == 1]\n",
    "        \n",
    "        scalar = pd.merge(scalar1, scalar2, on='Year')\n",
    "        scalar.dJews_x = scalar.dJews_x == 1\n",
    "        scalar.dJews_y = scalar.dJews_y == 1\n",
    "        \n",
    "        norm = np.max([scalar.dJews_x.sum(), scalar.dJews_y.sum()])\n",
    "        coicidence = (scalar.dJews_x * scalar.dJews_y)\n",
    "        count = coicidence.sum()\n",
    "    \n",
    "        if count != 0:\n",
    "            print(i, j, count, norm, scalar[coicidence].Year.values, end='\\r')\n",
    "            rate = count / norm\n",
    "        else:\n",
    "            rate = 0\n",
    "        results.append([i, j, count, norm, rate, scalar[coicidence].Year.values])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def compute_similarity_absence_presence():\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for i, j in combinations(places, 2):\n",
    "\n",
    "        print(i, places.max(), end='\\r')\n",
    "        \n",
    "        scalar1 = nodes[nodes.PlaceID == i][['Year', 'Juden']]\n",
    "        scalar2 = nodes[nodes.PlaceID == j][['Year', 'Juden']]\n",
    "        scalar = pd.merge(scalar1, scalar2, on='Year')\n",
    "        scalar = scalar.Juden_x == scalar.Juden_y\n",
    "        scalar = scalar.sum() / len(scalar)\n",
    "        results.append([i, j, scalar])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def compute_similarity_presence():\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for i, j in combinations(places, 2):\n",
    "\n",
    "        print(i, places.max(), end='\\r')\n",
    "        \n",
    "        scalar1 = nodes[nodes.PlaceID == i][['Year', 'Juden']]\n",
    "        scalar2 = nodes[nodes.PlaceID == j][['Year', 'Juden']]\n",
    "        scalar = pd.merge(scalar1, scalar2, on='Year')\n",
    "        scalar = scalar.Juden_x * scalar.Juden_y\n",
    "        scalar = scalar.sum() / len(scalar)\n",
    "        results.append([i, j, scalar])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb93778-6acc-4dcd-a13d-8202ab78d790",
   "metadata": {},
   "source": [
    "# Weighted network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56173ae2-dc78-43ba-8701-5902927c6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = weighted_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a3602e-e993-4c7d-9725-6ec7f7ae2517",
   "metadata": {},
   "source": [
    "## Edge weight distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbb65b6-e02b-481f-917a-481a981b6361",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [G.get_edge_data(u, v)['weight'] for u, v in G.edges()]\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.hist(weights, bins=np.arange(0, 1, 0.05), width=0.04)\n",
    "\n",
    "ax.set_xlabel('Edge weight')\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "hide_and_move_axis(ax)\n",
    "plt.savefig(OUTPUT_FOLDER / 'edge_weight_histrogram.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27befb5-2d8c-489f-9d65-73871487b11d",
   "metadata": {},
   "source": [
    "## Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ec3d4d-0221-41c0-9d2e-a5c3eb3c2f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "comms = nx.community.louvain_communities(G)\n",
    "T = np.sort([len(c) for c in comms])[-10]\n",
    "comms = [c for c in comms if len(c) >= T]\n",
    "assert len(comms) <= 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892cdfbd-bfa1-4722-a685-efe6bdc7b9b5",
   "metadata": {},
   "source": [
    "### Draw communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec843333-ea97-4bc4-9b83-d2ba8cf938a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_networkx_nodes(G, pos, node_size=12, node_color='k')\n",
    "\n",
    "for i, comm in enumerate(comms):\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=comm, node_size=15, node_color=colors[i])\n",
    "\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.025)\n",
    "plt.savefig(OUTPUT_FOLDER / 'communities.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77950bae-eb67-49c7-a6be-e86bf169b5ec",
   "metadata": {},
   "source": [
    "### Characterize communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8676c6-9bbe-4dfa-90bc-d870db435ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(2, 5, sharey=True, sharex=True, figsize=(8, 4))\n",
    "flatax = axarr.flatten()\n",
    "\n",
    "for i, community in enumerate(comms):\n",
    "    \n",
    "    in_edges = edges[edges['from'].isin(community) & edges['to'].isin(community)]\n",
    "    ruling_parties = in_edges.PartyID.str[0]\n",
    "    hist = ruling_parties.value_counts()\n",
    "\n",
    "    hist = pd.DataFrame(hist)\n",
    "    hist.sort_index(inplace=True)\n",
    "    \n",
    "    x = hist.index\n",
    "    y = hist['count'].values\n",
    "\n",
    "    y = y / y.sum()\n",
    "    \n",
    "    flatax[i].bar(x, y, color=colors[i])\n",
    "    hide_and_move_axis(flatax[i])\n",
    "\n",
    "axarr[1, 2].set_xlabel('Type of governing party')\n",
    "axarr[0, 0].set_ylabel('Relative frequency')\n",
    "axarr[1, 0].set_ylabel('Relative frequency')\n",
    "\n",
    "plt.savefig(OUTPUT_FOLDER / 'governing_parties.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc344a9-b561-4a96-a5d1-55dcc7c3456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    coincidences_df = pd.read_feather('computations/coincidences.feather')\n",
    "except:\n",
    "    coincidences = compute_coincidences()\n",
    "    coincidences_df = pd.DataFrame(coincidences, columns=['id1', 'id2', 'count', 'norm', 'rate', 'years'])\n",
    "    coincidences_df.set_index(['id1', 'id2'], inplace=True)\n",
    "    coincidences_df.to_feather('computations/coincidences.feather')\n",
    "else:\n",
    "    print('Read data from disk!')\n",
    "    \n",
    "try:\n",
    "    similarity_absence_presence_df = pd.read_feather('computations/similarity_absence_presence.feather')\n",
    "except:\n",
    "    similarity_absence_presence = compute_similarity_absence_presence()\n",
    "    similarity_absence_presence_df = pd.DataFrame(similarity_absence_presence, columns=['id1', 'id2', 'rate'])\n",
    "    similarity_absence_presence_df.set_index(['id1', 'id2'], inplace=True)\n",
    "    similarity_absence_presence_df.to_feather('computations/similarity_absence_presence.feather')\n",
    "else:\n",
    "    print('Read data from disk!')\n",
    "    \n",
    "try:\n",
    "    similarity_presence_df = pd.read_feather('computations/similarity_presence.feather')\n",
    "except:\n",
    "    similarity_presence = compute_similarity_presence()\n",
    "    similarity_presence_df = pd.DataFrame(similarity_presence, columns=['id1', 'id2', 'rate'])\n",
    "    similarity_presence_df.set_index(['id1', 'id2'], inplace=True)\n",
    "    similarity_presence_df.to_feather('computations/similarity_presence.feather')\n",
    "else:\n",
    "    print('Read data from disk!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59337da6-ddbe-4ba1-954d-b2ec7c76182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefficients(nodes, similarity, key='count'):\n",
    "\n",
    "    nodes = np.sort(list(nodes))\n",
    "    indices = list(combinations(nodes, 2))\n",
    "    return similarity[coincidences_df.index.isin(indices)][key]\n",
    "\n",
    "def cumulative(results):\n",
    "\n",
    "    x = np.sort(results)\n",
    "    N = len(results)\n",
    "    y = np.arange(N) / float(N) \n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1313c0dc-b5db-4c07-bf2f-91c68a6f5ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(2, 5, figsize=(10, 5), sharex=True, sharey=True)\n",
    "flatax = axarr.flatten()\n",
    "\n",
    "which = 'presence'\n",
    "\n",
    "if which == 'absencepresence':\n",
    "    SIMILARITY = similarity_absence_presence_df\n",
    "    KEY = 'rate'\n",
    "    axarr[1, 2].set_xlabel('Share of shared years with Jews present OR absent')\n",
    "elif which == 'presence':\n",
    "    SIMILARITY = similarity_presence_df\n",
    "    KEY = 'rate'\n",
    "    axarr[1, 2].set_xlabel('Share of shared years with Jews present')\n",
    "else:\n",
    "    assert False\n",
    "    \n",
    "for i in range(10):\n",
    "    print(i, end='\\r')\n",
    "    community = comms[i]\n",
    "    \n",
    "    for _ in range(200):\n",
    "        results = get_coefficients(np.random.choice(places, len(community), replace=False), SIMILARITY, KEY)\n",
    "        x, y = cumulative(results)\n",
    "        flatax[i].plot(x, y, alpha=0.1, c='k') \n",
    "    \n",
    "    results = get_coefficients(community, SIMILARITY, KEY)\n",
    "    x, y = cumulative(results)\n",
    "    \n",
    "    flatax[i].plot(x, y, lw=3, color=colors[i]) \n",
    "\n",
    "for ax in axarr[:, 0]:\n",
    "    ax.set_ylabel('Cumulative distribution function')\n",
    "\n",
    "#for ax in axarr[1]:\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_FOLDER / f'cumulative_distributions_{which}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b9fb5c-17e1-48a7-8766-9fe2d3bc0031",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMILARITY = coincidences_df\n",
    "KEY = 'count'\n",
    "\n",
    "f, axarr = plt.subplots(2, 5, figsize=(8, 4), sharex=True, sharey=True)\n",
    "flatax = axarr.flatten()\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    community = comms[i]\n",
    "    frequencies = []\n",
    "    \n",
    "    for _ in range(500):\n",
    "        results = get_coefficients(np.random.choice(places, len(community), replace=False), SIMILARITY, KEY)\n",
    "        frequencies.append((results > 0).mean())\n",
    "    flatax[i].hist(frequencies, color='k', bins=np.arange(0, 0.15, 0.005), alpha=0.25)\n",
    "    \n",
    "    results = get_coefficients(community, SIMILARITY, KEY)\n",
    "    results = (results > 0).mean()\n",
    "    flatax[i].axvline(results, color=colors[i], lw=3)\n",
    "    \n",
    "    frequencies = np.array(frequencies)\n",
    "    print(i, (frequencies > results).mean())\n",
    "\n",
    "    if (frequencies > results).mean() < 0.05:\n",
    "        flatax[i].text(0.13, 2, \"*\", size=20)\n",
    "\n",
    "for ax in axarr[:, 0]:\n",
    "    ax.set_ylabel('Count')\n",
    "\n",
    "for ax in axarr[1]:\n",
    "    ax.set_xlabel('Share of\\nco-explusion events')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_FOLDER / 'coexpulsion_frequencies.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67987ef-d8ee-42eb-a47f-620f46548d90",
   "metadata": {},
   "source": [
    "# Temporal networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444c4410-8e14-4369-80e4-d5c5ed907a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transitivity = np.zeros(len(years))\n",
    "clustering = np.zeros(len(years))\n",
    "degree = np.zeros(len(years))\n",
    "hamming_distance = np.zeros(len(years))\n",
    "hamming_distance[0] = np.nan\n",
    "n_components = np.zeros(len(years))\n",
    "size_largest_component = np.zeros(len(years))\n",
    "path_length = np.zeros(len(years))\n",
    "\n",
    "old_G = None\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(year, end='\\r')\n",
    "    \n",
    "    yearly_edges = singular_network(year, edges)\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(places)\n",
    "    G.add_edges_from(yearly_edges)\n",
    "    \n",
    "    transitivity[i] = nx.transitivity(G)\n",
    "    clustering[i] = nx.average_clustering(G)\n",
    "    degree[i] = 2 * G.number_of_edges() / G.number_of_nodes()\n",
    "\n",
    "    components = nx.connected_components(G)\n",
    "    n_components[i] = len(list(components))\n",
    "\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "    size_largest_component[i] = len(largest_cc)\n",
    "    #path_length[i] = nx.average_shortest_path_length(G.subgraph(largest_cc))\n",
    "    \n",
    "    if old_G is not None:\n",
    "        hamming_distance[i] = np.abs(nx.adjacency_matrix(G) - nx.adjacency_matrix(old_G)).sum()\n",
    "    old_G = G\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76972dd9-1664-4a1b-adc3-b793daa2a09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(6, 1, figsize=(6, 12))\n",
    "\n",
    "mask = years > 1100\n",
    "degree = degree[mask]\n",
    "transitivity = transitivity[mask]\n",
    "clustering = clustering[mask]\n",
    "hamming_distance = hamming_distance[mask]\n",
    "n_components = n_components[mask]\n",
    "size_largest_component = size_largest_component[mask]\n",
    "\n",
    "years = years[mask]\n",
    "\n",
    "transitions = years[1:][np.abs(np.diff(hamming_distance)) > 750]\n",
    "print(transitions)\n",
    "\n",
    "axarr[0].plot(years, degree)\n",
    "axarr[1].plot(years, transitivity)\n",
    "axarr[2].plot(years, clustering)\n",
    "axarr[3].plot(years, hamming_distance)\n",
    "axarr[4].plot(years, n_components)\n",
    "axarr[5].plot(years, size_largest_component / len(places))\n",
    "\n",
    "axarr[0].set_ylabel('Average degree')\n",
    "axarr[1].set_ylabel('Transitivity')\n",
    "axarr[2].set_ylabel('Average clustering')\n",
    "axarr[3].set_ylabel('Hamming distance')\n",
    "axarr[4].set_ylabel('# Components')\n",
    "axarr[5].set_ylabel('Share of largest\\ncomponent')\n",
    "\n",
    "for t in transitions:\n",
    "    for ax in axarr:\n",
    "        ax.axvline(t, c='k', alpha=0.5, ls=':')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
