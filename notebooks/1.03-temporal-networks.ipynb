{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320d2912-e065-45ae-bf1f-42bf2524234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import hydra\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize hydra and move to the root of the repository\n",
    "try:\n",
    "    hydra.initialize(version_base=None, config_path=\"../config/\")\n",
    "    CONFIG = hydra.compose(config_name=\"main.yaml\")\n",
    "    print('Initializing hydra')\n",
    "except:\n",
    "    print('Hydra already initalized!')\n",
    "else:\n",
    "    os.chdir('..')\n",
    "\n",
    "# Create an output folder in the root of the repository\n",
    "OUTPUT_FOLDER = Path('output/{0}'.format(datetime.datetime.now()))\n",
    "Path(OUTPUT_FOLDER).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6518d53f-58ab-4fa4-a87f-1d84f9edb242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "from src.utils.styling import hide_and_move_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ed686-935b-488a-922d-64c1551a272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalNetwork():\n",
    "\n",
    "    def __init__(self, nodes_file='data/01_raw/city_nodes_annual_all.csv', edges_file='data/01_raw/city_edges_annual_all.csv'):\n",
    "\n",
    "        self._nodes = pd.read_csv(nodes_file)\n",
    "        self._edges = pd.read_csv(edges_file)\n",
    "        self._edges['ruling_party_category'] = self._edges.PartyID.str[0]\n",
    "\n",
    "        self._position = self._nodes[['PlaceID', 'XCOORD', 'YCOORD']].drop_duplicates().set_index('PlaceID').to_dict('index')\n",
    "        self._position = {key: (value['XCOORD'], value['YCOORD']) for key, value in self._position.items()}\n",
    "\n",
    "        self._years = np.sort(self._edges.Year.unique())\n",
    "        self._places = self._nodes.PlaceID.unique()\n",
    "        \n",
    "        self.construct_networks()\n",
    "        self.compute_hamming_distance()\n",
    "        \n",
    "    def construct_networks(self):\n",
    "        \n",
    "        print('Constructing networks...')\n",
    "        self._networks = {}\n",
    "        \n",
    "        for year in self._years:\n",
    "            edges = self._edges[self._edges.Year == year]\n",
    "            edges = edges[['from', 'to']].values.tolist()\n",
    "            G = nx.Graph()\n",
    "            G.add_nodes_from(self._places)\n",
    "            G.add_edges_from(edges)\n",
    "            self._networks[year] = G\n",
    "\n",
    "    def compute_hamming_distance(self):\n",
    "\n",
    "        print('Computing hamming distance...')\n",
    "        self._hamming_distance = np.zeros(len(self._years))\n",
    "        self._hamming_distance[0] = np.nan\n",
    "        \n",
    "        for i, year in enumerate(self._years[1:]):\n",
    "            A1 = nx.adjacency_matrix(self.G(year))\n",
    "            A2 = nx.adjacency_matrix(self.G(year - 1))\n",
    "            self._hamming_distance[i+1] = np.abs(A1 - A2).sum()\n",
    "\n",
    "    def collapse(self, year0, year1, aggregation='binarize'):\n",
    "\n",
    "        A = nx.adjacency_matrix(self.G(year0))\n",
    "        for year in range(year0, year1+1):\n",
    "            A += nx.adjacency_matrix(self.G(year))\n",
    "\n",
    "        if aggregation == 'binarize':\n",
    "            A = (A >= 1).astype(int)\n",
    "        elif aggregate == 'always':\n",
    "            A = (A == A.max()).astype(int)\n",
    "        elif aggregate == 'majority':\n",
    "            A = (A >= (0.5 * A.max())).astype(int)\n",
    "        else:\n",
    "            assert False\n",
    "        \n",
    "        G = nx.Graph(A)\n",
    "        mapping = {i: place for i, place in enumerate(self._places)}\n",
    "        G = nx.relabel_nodes(G, mapping)\n",
    "        \n",
    "        return G\n",
    "\n",
    "    def get_city_names(self, node_list=[]):\n",
    "\n",
    "        return self._nodes[self._nodes.PlaceID.isin(node_list)][['PlaceID', 'PlaceName']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    def G(self, year):\n",
    "        return self._networks[year]\n",
    "\n",
    "    def years(self):\n",
    "        return self._years\n",
    "\n",
    "    def hamming_distance(self):\n",
    "        return self._hamming_distance\n",
    "\n",
    "    def position(self):\n",
    "        return self._position\n",
    "\n",
    "    def nodes_df(self):\n",
    "        return self._nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aab66a5-0315-4f6e-a80b-91481ac6d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = TemporalNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c06aa2a-b05b-415d-8716-b1115070d25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(4.5, 3))\n",
    "for year in network.years()[network.hamming_distance() > 400]:\n",
    "    if year not in np.arange(1001, 1551, 50):\n",
    "        print(year)\n",
    "        ax.axvline(year, c='r')\n",
    "    else:\n",
    "        ax.axvline(year, c='r', ls=':', alpha=0.3)\n",
    "    \n",
    "ax.plot(network.years(), network.hamming_distance(), c='k')\n",
    "hide_and_move_axis(ax)\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Hamming distance')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_FOLDER / 'hamming_distance.jpg', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ccb1fc-8627-4bfc-a8c1-cdd68b7c0ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate = 'majority'\n",
    "\n",
    "G1 = network.collapse(1351, 1354, aggregation=aggregate)\n",
    "G2 = network.collapse(1355, 1400, aggregation=aggregate)\n",
    "G3 = network.collapse(1401, 1414, aggregation=aggregate)\n",
    "G4 = network.collapse(1415, 1417, aggregation=aggregate)\n",
    "G5 = network.collapse(1418, 1450, aggregation=aggregate)\n",
    "\n",
    "networks = [(G1, 1351, 1354), (G2, 1355, 1400), (G3, 1401, 1414), (G4, 1415, 1417), (G5, 1418, 1450)]\n",
    "\n",
    "f, axarr = plt.subplots(4, 4, figsize=(12, 14))\n",
    "\n",
    "for i in range(len(networks) - 1):\n",
    "\n",
    "    axs = axarr[i]\n",
    "    \n",
    "    G_1, year0_1, year1_1 = networks[i]\n",
    "    G_2, year0_2, year1_2 = networks[i+1]\n",
    "    \n",
    "    added_edges = G_2.copy()\n",
    "    added_edges.remove_edges_from(G_1.edges())\n",
    "\n",
    "    removed_edges = G_1.copy()\n",
    "    removed_edges.remove_edges_from(G_2.edges())\n",
    "\n",
    "    for i, G in enumerate([G_1, G_2]):\n",
    "        nx.draw_networkx_nodes(G, network.position(), node_size=4, node_color='k', ax=axs[i])\n",
    "        nx.draw_networkx_edges(G, network.position(), alpha=0.05, ax=axs[i])\n",
    "    \n",
    "    nx.draw_networkx_nodes(added_edges, network.position(), node_size=4, node_color='k', ax=axs[2])\n",
    "    nx.draw_networkx_edges(added_edges, network.position(), alpha=0.1, edge_color='b', ax=axs[2])\n",
    "    \n",
    "    nx.draw_networkx_nodes(removed_edges, network.position(), node_size=4, node_color='k', ax=axs[3])\n",
    "    nx.draw_networkx_edges(removed_edges, network.position(), alpha=0.1, edge_color='r', ax=axs[3])\n",
    "\n",
    "    axs[0].set_title(f'{year0_1} - {year1_1}')\n",
    "    axs[1].set_title(f'{year0_2} - {year1_2}')\n",
    "    axs[2].set_title(str(len(added_edges.edges())) + ' added edges')\n",
    "    axs[3].set_title(str(len(removed_edges.edges())) + ' removed edges')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_FOLDER / f'temporal_network_changes_{aggregate}.jpg', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1689ea0a-69cd-4129-8425-ad8026993a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(networks) - 1):\n",
    "\n",
    "    G_1, year0_1, year1_1 = networks[i]\n",
    "    G_2, year0_2, year1_2 = networks[i+1]\n",
    "\n",
    "    if year1_1 == 1400:\n",
    "        continue\n",
    "\n",
    "    added_edges = G_2.copy()\n",
    "    added_edges.remove_edges_from(G_1.edges())\n",
    "\n",
    "    removed_edges = G_1.copy()\n",
    "    removed_edges.remove_edges_from(G_2.edges())\n",
    "\n",
    "    for edges, y0, y1, action in ((added_edges, year0_2, year1_2, 'added'), (removed_edges, year0_1, year1_1, 'removed')):\n",
    "        \n",
    "        most_common = np.array(sorted(edges.degree, key=lambda x: x[1], reverse=True))\n",
    "        most_common = pd.DataFrame(most_common, columns=['PlaceID', 'changed_edges'])\n",
    "        print(f'Total number of edge changes from {year1_1} to {year0_2}:', most_common.changed_edges.sum() / 2)\n",
    "\n",
    "        names = network.get_city_names(most_common.PlaceID)\n",
    "        most_common = pd.merge(names, most_common, on='PlaceID')\n",
    "        most_common.sort_values(by='changed_edges', ascending=False, inplace=True)\n",
    "        most_common = most_common[most_common.changed_edges >= 10]\n",
    "\n",
    "        affected_parties_df = []\n",
    "\n",
    "        for node_id in most_common.PlaceID:\n",
    "        \n",
    "            to = [n for n in edges.neighbors(node_id)]\n",
    "            edge = network._edges[(network._edges['from'] == node_id) & network._edges['to'].isin(to) & network._edges['Year'].between(y0, y1)]\n",
    "            affected_parties = edge.sort_values(['to','Year'],ascending=False).groupby('to').head(1)['PartyID'].value_counts()\n",
    "    \n",
    "            result = ''\n",
    "            for row in affected_parties.items():\n",
    "                result += str(row[0]) + ': ' + str(row[1]) + '; '\n",
    "            result = result[:-2]\n",
    "\n",
    "            affected_parties_df.append((node_id, result))\n",
    "\n",
    "        affected_parties_df = pd.DataFrame(affected_parties_df, columns=['PlaceID', 'affected_parties'])\n",
    "        most_common = pd.merge(most_common, affected_parties_df, on='PlaceID')\n",
    "\n",
    "        print(f'Cities with more than 10 edges {action} from {year1_1} to {year0_2}:', end='\\n\\n')\n",
    "        print(most_common.to_markdown(index=False), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b0d2af-b63d-4f98-9cb6-8c3a52e0781b",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ab3ef7-8f8c-413d-9d33-252bd45ac267",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def temporal_network_analysis(edges, places, years, min_community_size=10):\n",
    "\n",
    "    results = []\n",
    "    old_G = None\n",
    "    hamming_distance = np.nan\n",
    "    \n",
    "    for i, year in enumerate(years):\n",
    "        print(year, end='\\r')\n",
    "        \n",
    "        G = singular_network(year, edges, places)\n",
    "        \n",
    "        components = nx.connected_components(G)\n",
    "        n_components = len(list(components))\n",
    "\n",
    "        largest_cc = max(nx.connected_components(G), key=len)\n",
    "        size_largest_component = len(largest_cc)\n",
    "\n",
    "        communities = nx.community.louvain_communities(G)\n",
    "        large_communites = sum([len(community) >= min_community_size for community in communities])\n",
    "\n",
    "        if old_G is not None:\n",
    "            hamming_distance = np.abs(nx.adjacency_matrix(G) - nx.adjacency_matrix(old_G)).sum()\n",
    "        old_G = G\n",
    "        \n",
    "        result = [\n",
    "            year,\n",
    "            nx.transitivity(G),\n",
    "            nx.average_clustering(G),\n",
    "            2 * G.number_of_edges() / G.number_of_nodes(),\n",
    "            n_components,\n",
    "            size_largest_component / len(places), \n",
    "            nx.average_shortest_path_length(G.subgraph(largest_cc)),\n",
    "            communities,\n",
    "            hamming_distance,\n",
    "            large_communites\n",
    "        ]\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    columns = [\n",
    "        'year', \n",
    "        'transitivity', \n",
    "        'clustering', \n",
    "        'degree', \n",
    "        'nComponents', \n",
    "        'shareLargestComponent', \n",
    "        'pathlength', \n",
    "        'communities', \n",
    "        'hammingdistance', \n",
    "        'nMeaningfulCommunities'\n",
    "    ]\n",
    "\n",
    "    results = pd.DataFrame(results, columns=columns)\n",
    "    results.set_index('year', inplace=True)\n",
    "    \n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429c42ac-8576-41e3-bc00-9531ebfe29a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = temporal_network_analysis(places=PLACES, edges=EDGES, years=YEARS)\n",
    "\n",
    "RESULTS['transition1'] = RESULTS.hammingdistance > (RESULTS.hammingdistance.mean() + RESULTS.hammingdistance.std())\n",
    "RESULTS['transition2'] = RESULTS.shareLargestComponent.diff().abs() > 0.05\n",
    "RESULTS['transition3'] = RESULTS.clustering.diff().abs() > 0.02\n",
    "RESULTS['transition'] = RESULTS.transition1 | RESULTS.transition2 | RESULTS.transition3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc642d3-9357-43f4-b432-6546816ce7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_SEQUENCE = [\n",
    "    'degree', \n",
    "    'transitivity', \n",
    "    'clustering', \n",
    "    'hammingdistance', \n",
    "    'nComponents', \n",
    "    'shareLargestComponent', \n",
    "    'pathlength', \n",
    "    'nMeaningfulCommunities'\n",
    "]\n",
    "\n",
    "f, axarr = plt.subplots(4, 2, figsize=(10, 8))\n",
    "axarr = axarr.flatten()\n",
    "\n",
    "print(RESULTS[RESULTS.transition].index)\n",
    "\n",
    "for ax, measure in zip(axarr, PLOT_SEQUENCE):\n",
    "    ax.plot(RESULTS[measure])\n",
    "    ax.set_ylabel(measure)\n",
    "\n",
    "for t in RESULTS[RESULTS.transition].index:\n",
    "    for ax in axarr:\n",
    "        if ((t-1) % 50) == 0:\n",
    "            ax.axvline(t, c='k', alpha=0.5, ls=':')\n",
    "        else:\n",
    "            ax.axvline(t, c='r', alpha=0.5, ls=':')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_FOLDER / 'temporal_networks_yearly.png')\n",
    "\n",
    "print([y for y in RESULTS[RESULTS.transition].index if ((y-1) % 50) != 0])\n",
    "print([y for y in RESULTS[RESULTS.transition].index if ((y-1) % 50) == 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
